{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Animacy in German Folktales\n",
    "\n",
    "This notebook contains the reproducible code examples and analyses for the paper *\"Animacy in German Folktales\"* submitted in proceedings of CHR 2024: Computational Humanities Research Conference, 2024, Aarhus, Denmark.\n",
    "\n",
    "**Authors:** Julian Häußler, Janis von Keitz, Evelyn Gius\n",
    "\n",
    "**Institution:** *fortext lab, Technical University of Darmstadt, Germany*\n",
    "\n",
    "**Reference:** Häußler, J., von Keitz, J., Gius, E. (2024). *Animacy in German Folktales*. CHR 2024: Computational Humanities Research Conference, December 4 – 6, 2024, Aarhus, Denmark. https://ceur-ws.org/Vol-3834/paper90.pdf.\n",
    "\n",
    "**GitHub Repository:** https://github.com/forTEXT/Animacy_in_German_Folktales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook 01: Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-24 16:02:56 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbd9f33df6b54f4799edf91a2f43680c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.9.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-24 16:02:56 INFO: Downloaded file to C:\\Users\\jvonk\\stanza_resources\\resources.json\n",
      "2024-11-24 16:02:58 INFO: Loading these models for language: de (German):\n",
      "============================\n",
      "| Processor | Package      |\n",
      "----------------------------\n",
      "| tokenize  | gsd          |\n",
      "| mwt       | gsd          |\n",
      "| pos       | gsd_charlm   |\n",
      "| lemma     | gsd_nocharlm |\n",
      "| depparse  | gsd_charlm   |\n",
      "| ner       | germeval2014 |\n",
      "============================\n",
      "\n",
      "2024-11-24 16:02:58 INFO: Using device: cpu\n",
      "2024-11-24 16:02:58 INFO: Loading: tokenize\n",
      "c:\\Users\\jvonk\\.pyenv\\pyenv-win\\versions\\3.11.9\\Lib\\site-packages\\stanza\\models\\tokenization\\trainer.py:82: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-11-24 16:03:00 INFO: Loading: mwt\n",
      "c:\\Users\\jvonk\\.pyenv\\pyenv-win\\versions\\3.11.9\\Lib\\site-packages\\stanza\\models\\mwt\\trainer.py:201: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-11-24 16:03:00 INFO: Loading: pos\n",
      "c:\\Users\\jvonk\\.pyenv\\pyenv-win\\versions\\3.11.9\\Lib\\site-packages\\stanza\\models\\pos\\trainer.py:139: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "c:\\Users\\jvonk\\.pyenv\\pyenv-win\\versions\\3.11.9\\Lib\\site-packages\\stanza\\models\\common\\pretrain.py:56: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(self.filename, lambda storage, loc: storage)\n",
      "c:\\Users\\jvonk\\.pyenv\\pyenv-win\\versions\\3.11.9\\Lib\\site-packages\\stanza\\models\\common\\char_model.py:271: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-11-24 16:03:00 INFO: Loading: lemma\n",
      "c:\\Users\\jvonk\\.pyenv\\pyenv-win\\versions\\3.11.9\\Lib\\site-packages\\stanza\\models\\lemma\\trainer.py:239: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-11-24 16:03:01 INFO: Loading: depparse\n",
      "c:\\Users\\jvonk\\.pyenv\\pyenv-win\\versions\\3.11.9\\Lib\\site-packages\\stanza\\models\\depparse\\trainer.py:194: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-11-24 16:03:01 INFO: Loading: ner\n",
      "c:\\Users\\jvonk\\.pyenv\\pyenv-win\\versions\\3.11.9\\Lib\\site-packages\\stanza\\models\\ner\\trainer.py:197: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-11-24 16:03:02 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "\n",
    "import pandas as pd\n",
    "import os \n",
    "import json\n",
    "import re\n",
    "import stanza\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "# Initialize Stanza Pipeline\n",
    "nlp = stanza.Pipeline(lang='de', processors='tokenize,mwt,pos,lemma,depparse,ner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set input and output folder\n",
    "input_folder = 'input/typical_animacy_corpus' # Input folder with TXT files\n",
    "output_folder = 'intermediate/annotations_typical' # Output folder to store annotated texts as JSON files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Annotations Dataframe\n",
    "This step is based on a Catma Query Export. In the Analyze section of Catma you can filter all your annotations for a specific tag (e.g. animate). The result needs then to be exported *Flat as CSV*. The export is put in the working folder and renamed ```catma_export.csv```. After that you can continue with the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the catma export output as dataframe \n",
    "catma_export_path = f'{input_folder}/catma_export.csv'\n",
    "annotations_df = pd.read_csv(catma_export_path, sep = ';')\n",
    "\n",
    "# Seletc the essential columns from the catma export file\n",
    "annotations_df = annotations_df.iloc[:, [2, 3, 4, 6, 7]]\n",
    "\n",
    "# Name these columns\n",
    "annotations_df.columns = ['title', 'chars', 'annotation', 'start', 'end']\n",
    "\n",
    "# Save df as CSV\n",
    "annotations_df_path = f'{output_folder}/annotations.csv'\n",
    "annotations_df.to_csv(annotations_df_path, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To match the the TXT file with the correct entries in the Dataframe you need to make sure that there is a column in the Dataframe that contains an identifier for the txt file. In our case we used the following code to split the ```title``` column in an ```id``` and a ```name``` column, since our names for the text used in Catma consisted of a number and the name of the story. The TXT files are just named accordingly to this numbers.\n",
    "\n",
    "For example:  We named a story *171. Der Zaunkönig* in Catma so this is the ```title``` in the Dataframe. We split the column so the ID of the story is *171*. The matching TXT file is named ```171.txt```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Only use to split ID and name of story if your titles are in the format as mentioned above\n",
    "\n",
    "# Split column title in columns id and name\n",
    "annotations_df[['id', 'name']] = annotations_df['title'].str.split('.', expand=True)\n",
    "\n",
    "# Delete column 'title'\n",
    "annotations_df.drop('title', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you do not have an ID but you have unambiguous titles in Catma, you can also use the ```title``` column to identify the TXT files. Just update the code below accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the name of the column that you want to identify your txt files with\n",
    "identifier = 'id' # Change to 'title' or other to use this column to identify txt files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Annotations\n",
    "\n",
    "Sometimes the problem might occur, that the indices from Catma does not match the TXT files exaclty. This function searches for the correct annotation starting from the index given by Catma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find annotation in text backwards (in case of an offset between Catma annotation indices and the txt file)\n",
    "def find_annotation(text: str, annotation: str, start_index: int) -> tuple:\n",
    "    lower_text = text.lower()\n",
    "    lower_annotation = annotation.lower()\n",
    "    annotation_length = len(lower_annotation)\n",
    "\n",
    "    # Search backwards\n",
    "    for i in range(start_index, -1, -1):\n",
    "        if lower_text[i:i + annotation_length] == lower_annotation:\n",
    "            return i, i + annotation_length\n",
    "\n",
    "    # Search forward\n",
    "    for i in range(start_index, len(lower_text) - annotation_length + 1):\n",
    "        if lower_text[i:i + annotation_length] == lower_annotation:\n",
    "            return i, i + annotation_length\n",
    "\n",
    "    # If not found\n",
    "    return -1, -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if annotations in Dataframe match indices\n",
    "\n",
    "# Iterate over all TXT files in input folder\n",
    "for filename in os.listdir(input_folder):\n",
    "    file_path = os.path.join(input_folder, filename)\n",
    "\n",
    "    # Skip all files that are not txt\n",
    "    if not filename.endswith('.txt'): continue\n",
    "\n",
    "    # Read text from file\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    \n",
    "    # Filter entries for actual story with ID\n",
    "    txt_id = int(filename.replace('.txt', '')) # ID of the actual story\n",
    "    filtered_df = annotations_df[annotations_df[identifier] == txt_id]\n",
    "    \n",
    "    # Check if Dataframe has entries for actual text\n",
    "    if filtered_df.empty: continue\n",
    "    \n",
    "    # Iterate over every annotation in actual story\n",
    "    for index, row in filtered_df.iterrows():\n",
    "        start_index = int(row['start'])\n",
    "        end_index = int(row['end'])\n",
    "        annotation_actual = text[start_index:end_index].strip()\n",
    "        annotation_check = row['annotation']\n",
    "\n",
    "        # Check if extracted annotations from start and end index matches annotation from table\n",
    "        if annotation_check != annotation_actual: \n",
    "            \n",
    "            # raise ValueError(f'Die Annotationen {annotation} und {annotation_check} stimmen nicht überein.')\n",
    "            start_index, end_index = find_annotation(text, annotation_check, start_index)\n",
    "            annotation = text[start_index:end_index].strip()\n",
    "\n",
    "            # Update indices in annotations_df\n",
    "            annotations_df.at[index, 'start'] = start_index\n",
    "            annotations_df.at[index, 'end'] = end_index\n",
    "\n",
    "# Save df as CSV\n",
    "annotations_df_path = f'{output_folder}/annotations.csv'\n",
    "annotations_df.to_csv(annotations_df_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Annotation Files\n",
    "\n",
    "In this step, we create annotations from the indices in the annotations_df. Therefore all indices of the annotations are looked up in the txt files. For each texts we also get NER, POS, Lemmas and Tokens. Everything is collected in a Dataframe for each text and then stored as a JSON file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [02:00<00:00, 17.19s/it]\n"
     ]
    }
   ],
   "source": [
    "# Load annotation df \n",
    "annotations_df_path = f'{output_folder}/annotations.csv'\n",
    "annotations_df = pd.read_csv(annotations_df_path)\n",
    "\n",
    "# Create output folder if it does not already exist\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "# Iterate over all TXT files in the input folder\n",
    "for filename in tqdm(os.listdir(input_folder)):\n",
    "    file_path = os.path.join(input_folder, filename)\n",
    "\n",
    "    # Skip all files that are not txt\n",
    "    if not filename.endswith('.txt'): continue\n",
    "\n",
    "    # Filter entries for actual story with ID\n",
    "    txt_id = int(filename.replace('.txt', '')) # ID of the actual story\n",
    "    filtered_df = annotations_df[annotations_df[identifier] == txt_id]\n",
    "    \n",
    "    # Check if annotations exist for actual ID\n",
    "    if filtered_df.shape[0] == 0:\n",
    "        continue\n",
    "        \n",
    "    # Read text from txt file\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "\n",
    "    # Create NLP Object and get tokens, lemmas, and further tags\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    tokens = []\n",
    "    lemmas = []\n",
    "    pos_tags = []\n",
    "    dep_tags = []\n",
    "    ner_tags = []\n",
    "    start_chars = []\n",
    "    end_chars = []\n",
    "\n",
    "    for sent in doc.sentences:\n",
    "        for token in sent.tokens:\n",
    "            # Information from token object\n",
    "            tokens.append(token.text)\n",
    "            ner_tags.append(token.ner)\n",
    "            start_chars.append(token.start_char)\n",
    "            end_chars.append(token.end_char)\n",
    "            # Information from word object\n",
    "            lemmas.append(token.words[0].lemma)\n",
    "            pos_tags.append(token.words[0].upos)\n",
    "            dep_tags.append(token.words[0].deprel)\n",
    "\n",
    "\n",
    "    # Create Dataframe with all Information for actual text\n",
    "\n",
    "    df_text_annotated = pd.DataFrame(columns=['tokens','animate_tags','lemmas','pos_tags','dep_tags','ner_tags','start_chars','end_chars'])\n",
    "\n",
    "    df_text_annotated['tokens']=tokens\n",
    "    df_text_annotated['lemmas']=lemmas\n",
    "    df_text_annotated['pos_tags']=pos_tags\n",
    "    df_text_annotated['dep_tags']=dep_tags\n",
    "    df_text_annotated['ner_tags']=ner_tags\n",
    "    df_text_annotated['start_chars']=start_chars\n",
    "    df_text_annotated['end_chars']=end_chars\n",
    "    \n",
    "\n",
    "    # Annotate text\n",
    "    for index,row in df_text_annotated.iterrows():\n",
    "        is_animate = False \n",
    "        start_pos = row['start_chars']\n",
    "        end_pos = row['end_chars']\n",
    "        for ann_start, ann_end in zip(filtered_df['start'], filtered_df['end']):\n",
    "            if start_pos >= ann_start and end_pos <= ann_end:\n",
    "                is_animate = True\n",
    "                break\n",
    "        annotation_type = 'animate' if is_animate else 'inanimate'\n",
    "        df_text_annotated.loc[index, 'animate_tags'] = annotation_type\n",
    "\n",
    "    # Save df with annotations as JSON\n",
    "    target_path_annotations = os.path.join(output_folder, filename.replace('.txt', '_annotations.json'))\n",
    "    with open(target_path_annotations, 'w', encoding='utf-8') as json_file:\n",
    "        df_text_annotated.to_json(json_file, force_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inter Annotator Agreement\n",
    "\n",
    "In the following Code. We tested the IAA of two Texts measuring the Cohens Kappa Coefficient. For this we first load the tags of the two annotations for each text in a separate list. Wen then calculate Cohens Kappa for each text and lastly the average for both texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set input folder with IAA Annotations\n",
    "input_iaa = 'intermediate/annotations_iaa'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load tags from JSON File as list\n",
    "def load_tags(filepath):\n",
    "    with open(filepath) as f:\n",
    "        data = json.load(f)\n",
    "        return [token[1] for token in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tags for each annotation of both texts as lists\n",
    "tags_6_A1 = load_tags(f'{input_iaa}/A1/6_annotations.json')\n",
    "tags_6_A2 = load_tags(f'{input_iaa}/A2/6_annotations.json')\n",
    "tags_10_A1 = load_tags(f'{input_iaa}/A1/10_annotations.json')\n",
    "tags_10_A2 = load_tags(f'{input_iaa}/A2/10_annotations.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kappa for text 6: 0.8839836032587045\n",
      "Kappa for text 10: 0.8555919798734466\n",
      "Average Kappa: 0.8697877915660756\n"
     ]
    }
   ],
   "source": [
    "# Calculate Cohen's Kappa for both texts\n",
    "kappa_6 = cohen_kappa_score(tags_6_A1, tags_6_A2)\n",
    "kappa_10 = cohen_kappa_score(tags_10_A1, tags_10_A2)\n",
    "\n",
    "# Print individual and Average Kappa Scores\n",
    "print(f\"Kappa for text 6: {kappa_6}\")\n",
    "print(f\"Kappa for text 10: {kappa_10}\")\n",
    "print(f\"Average Kappa: {(kappa_6 + kappa_10) / 2}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
